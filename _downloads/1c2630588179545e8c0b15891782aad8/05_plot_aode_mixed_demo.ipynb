{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# The Power of Mixed Data: Context-Dependent Logic\n\nThis example demonstrates why **AnDE** shines with mixed data types\n(Continuous + Categorical) compared to Naive Bayes.\n\nWe simulate a system with a **Continuous Feature (X)** and a\n**Categorical \"Mode\" (C)**. The definition of \"Anomaly\" (Class 1)\nchanges completely depending on the mode.\n\n**Result:**\n- **MixedNB (Naive):** Fails. It smears probability across the range.\n- **AnDE (n=1):** Succeeds. It learns distinct boundaries per mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: The scikit-bayes Developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nfrom skbn.ande import AnDE\nfrom skbn.mixed_nb import MixedNB\n\n# Suppress discretization warnings for this demo\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# --- 1. Generate Mixed Dataset ---\nnp.random.seed(42)\nn_samples = 3000\n\n# Feature 0: Continuous (Standard Normal)\nX_cont = np.random.randn(n_samples, 1) * 1.5\n# Feature 1: Categorical (0, 1, 2)\nX_cat = np.random.randint(0, 3, size=(n_samples, 1))\n\n# Stack them\nX = np.hstack([X_cont, X_cat])\n\n# Logic:\ny = np.zeros(n_samples, dtype=int)\n# Mode 0: Positive High (> 1)\nmask_0 = (X_cat.flatten() == 0) & (X_cont.flatten() > 1.0)\ny[mask_0] = 1\n# Mode 1: Positive Low (< -1)\nmask_1 = (X_cat.flatten() == 1) & (X_cont.flatten() < -1.0)\ny[mask_1] = 1\n# Mode 2: Positive Middle (-0.5 < x < 0.5)\nmask_2 = (X_cat.flatten() == 2) & (np.abs(X_cont.flatten()) < 0.5)\ny[mask_2] = 1\n\n# --- 2. Fit Models ---\nprint(\"Training models...\")\n\nmnb = MixedNB()\nmnb.fit(X, y)\n\nande = AnDE(n_dependence=1)\nande.fit(X, y)\n\nmodels = [mnb, ande]\ntitles = [\"Naive Bayes (MixedNB)\", \"AnDE (n=1, Context-Aware)\"]\n\n# --- 3. Visualization ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n\n# Grid for plotting\n# We need CENTERS for prediction, but EDGES for pcolormesh plotting\nn_points = 200\nx_centers = np.linspace(-4, 4, n_points)\n\n# Create edges for X (must be length n_points + 1)\nstep = x_centers[1] - x_centers[0]\nx_edges = np.concatenate([x_centers - step / 2, [x_centers[-1] + step / 2]])\n\n# Y edges are manual (categorical bands)\ny_edges = np.array([-0.5, 0.5, 1.5, 2.5])\n\n# Prediction loop\nfor ax, model, title in zip(axes, models, titles):\n    acc = accuracy_score(y, model.predict(X))\n\n    # Construct probability map\n    prob_map = np.zeros((3, n_points))\n\n    for cat_val in [0, 1, 2]:\n        # Create a batch of data: [x_centers, constant_cat]\n        batch_cat = np.full((n_points, 1), cat_val)\n        batch_X = np.hstack([x_centers.reshape(-1, 1), batch_cat])\n\n        # Predict\n        probs = model.predict_proba(batch_X)[:, 1]\n        prob_map[cat_val, :] = probs\n\n    # Plot Heatmap - VIRIDIS\n    # cmap='viridis': Purple (0.0) -> Yellow (1.0)\n    pcm = ax.pcolormesh(\n        x_edges,\n        y_edges,\n        prob_map,\n        cmap=\"viridis\",\n        vmin=0,\n        vmax=1,\n        shading=\"flat\",\n        alpha=0.8,\n    )\n\n    # Overlay real data points\n    mask_sub = np.random.choice(n_samples, 400, replace=False)\n    X_sub = X[mask_sub]\n    y_sub = y[mask_sub]\n\n    # Jitter Y for visibility\n    y_jitter = X_sub[:, 1] + np.random.uniform(-0.2, 0.2, size=len(X_sub))\n\n    # Class 0 -> Indigo Circle (Low prob)\n    ax.scatter(\n        X_sub[y_sub == 0, 0],\n        y_jitter[y_sub == 0],\n        c=\"indigo\",\n        marker=\"o\",\n        s=30,\n        alpha=0.6,\n        edgecolors=\"w\",\n        linewidth=0.8,\n        label=\"Class 0\",\n    )\n\n    # Class 1 -> Gold Triangle (High prob)\n    # Black edge for contrast against yellow background\n    ax.scatter(\n        X_sub[y_sub == 1, 0],\n        y_jitter[y_sub == 1],\n        c=\"gold\",\n        marker=\"^\",\n        s=30,\n        alpha=0.8,\n        edgecolors=\"k\",\n        linewidth=0.5,\n        label=\"Class 1\",\n    )\n\n    ax.set_title(f\"{title}\\nAccuracy: {acc:.3f}\")\n    ax.set_xlabel(\"Continuous Feature Value\")\n    ax.set_yticks([0, 1, 2])\n    ax.set_yticklabels([\"Mode 0\\n(High)\", \"Mode 1\\n(Low)\", \"Mode 2\\n(Band)\"])\n\n    # Reference lines\n    ax.vlines(1.0, -0.5, 0.5, colors=\"black\", linestyles=\"--\", alpha=0.5)\n    ax.vlines(-1.0, 0.5, 1.5, colors=\"black\", linestyles=\"--\", alpha=0.5)\n    ax.vlines([-0.5, 0.5], 1.5, 2.5, colors=\"black\", linestyles=\"--\", alpha=0.5)\n\naxes[0].set_ylabel(\"Categorical Feature (Context)\")\ncbar = fig.colorbar(\n    pcm, ax=axes.ravel().tolist(), orientation=\"horizontal\", fraction=0.05, pad=0.15\n)\ncbar.set_label(\"Predicted Probability of Anomaly (Class 1)\")\n\nfig.suptitle(\n    \"Mixed Data Capabilities: Adapting Continuous Boundaries per Category\", fontsize=16\n)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}