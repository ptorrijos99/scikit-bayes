{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Interpreting ALR: Automatic Feature Selection via Weights\n\nThis example demonstrates the \"Embedded Feature Selection\" capability of\nHybrid AnDE models (ALR).\n\n**The Result:**\nBy ignoring noise, ALR achieves significantly lower Log Loss and higher Accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: The scikit-bayes Developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import train_test_split\n\nfrom skbn.ande import ALR, AnDE\n\n# --- 1. Generate Dataset with Heavy Noise ---\nnp.random.seed(42)\nn_samples = 20000  # Large sample size to allow convergence of weights\nn_features = 10\n\n# A. Informative Features (Mixed Type)\nX_cat = np.random.randint(0, 3, size=(n_samples, 1))\nX_cont = np.random.randn(n_samples, 1)\n\n# Logic: Cat=0 & X>0.5 OR Cat=1 & X<-0.5 -> Class 1\ny = np.zeros(n_samples, dtype=int)\nmask_0 = (X_cat.flatten() == 0) & (X_cont.flatten() > 0.5)\nmask_1 = (X_cat.flatten() == 1) & (X_cont.flatten() < -0.5)\ny[mask_0 | mask_1] = 1\n\n# B. Noise Features\nX_noise = np.random.randn(n_samples, 8)\nX = np.hstack([X_cat, X_cont, X_noise])\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# --- 2. Fit and Evaluate ---\nprint(\"Training models...\")\n\n# Model 1: AnDE\nande = AnDE(n_dependence=1)\nande.fit(X_train, y_train)\nacc_ande = accuracy_score(y_test, ande.predict(X_test))\nll_ande = log_loss(y_test, ande.predict_proba(X_test))\n\n# Model 2: ALR (Higher regularization for visualization sparsity)\nalr = ALR(n_dependence=1, l2_reg=0.05, max_iter=200)\nalr.fit(X_train, y_train)\nacc_alr = accuracy_score(y_test, alr.predict(X_test))\nll_alr = log_loss(y_test, alr.predict_proba(X_test))\n\n# --- 3. Visualization ---\nfeatures = [f\"F{i}\\n(Signal)\" if i < 2 else f\"F{i}\\n(Noise)\" for i in range(n_features)]\nindices = np.arange(n_features)\n\nande_weights = np.ones(n_features)\n\n# Detect if we have weights per class (Level 3) or simple weights (Level 1)\nn_models = len(alr.ensemble_)\n\nif alr.learned_weights_.size == n_models:\n    # Level 1: One weight per feature. Use directly.\n    alr_weights = alr.learned_weights_\nelse:\n    # Level 3: Weights per class. Reshape and average.\n    n_classes = len(alr.classes_)\n    weights_matrix = alr.learned_weights_.reshape(n_models, n_classes)\n    alr_weights = np.mean(weights_matrix, axis=1)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n\n\n# Helper for info box (MOVED TO RIGHT)\ndef add_score_box(ax, acc, ll):\n    textstr = \"\\n\".join(\n        (r\"$\\bf{Performance (Test):}$\", f\"Accuracy: {acc:.3f}\", f\"Log Loss: {ll:.3f}\")\n    )\n    props = dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9, edgecolor=\"gray\")\n    ax.text(\n        0.95,\n        0.95,\n        textstr,\n        transform=ax.transAxes,\n        fontsize=12,\n        verticalalignment=\"top\",\n        horizontalalignment=\"right\",\n        bbox=props,\n    )\n\n\n# Plot AnDE (Generative)\naxes[0].bar(indices, ande_weights, color=\"lightgray\", edgecolor=\"gray\")\naxes[0].set_title(\"AnDE (Generative)\\nStrategy: Uniform Attention\", fontsize=14)\naxes[0].set_xlabel(\"Feature (as Super-Parent)\", fontsize=12)\naxes[0].set_ylabel(\"Weight Magnitude\", fontsize=12)\naxes[0].set_xticks(indices)\naxes[0].set_xticklabels(features, rotation=45, ha=\"right\")\n\n# Highlight Signal Features with GOLD (to show ground truth importance)\naxes[0].patches[0].set_facecolor(\"gold\")\naxes[0].patches[0].set_edgecolor(\"orange\")\naxes[0].patches[1].set_facecolor(\"gold\")\naxes[0].patches[1].set_edgecolor(\"orange\")\n\nadd_score_box(axes[0], acc_ande, ll_ande)\n\n# Plot ALR (Hybrid)\n# Use INDIGO for learned weights\nbars = axes[1].bar(indices, alr_weights, color=\"indigo\", edgecolor=\"black\", alpha=0.8)\naxes[1].set_title(\"ALR (Hybrid)\\nStrategy: Learned Attention\", fontsize=14)\naxes[1].set_xlabel(\"Feature (as Super-Parent)\", fontsize=12)\naxes[1].set_xticks(indices)\naxes[1].set_xticklabels(features, rotation=45, ha=\"right\")\n\nadd_score_box(axes[1], acc_alr, ll_alr)\n\n# Annotate values on ALR\nfor bar in bars:\n    height = bar.get_height()\n    if height > 0.1:\n        axes[1].annotate(\n            f\"{height:.2f}\",\n            xy=(bar.get_x() + bar.get_width() / 2, height),\n            xytext=(0, 3),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontweight=\"bold\",\n            color=\"indigo\",\n        )\n\nfig.suptitle(\"Impact of Noise on Model Weights and Performance\", fontsize=18)\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}