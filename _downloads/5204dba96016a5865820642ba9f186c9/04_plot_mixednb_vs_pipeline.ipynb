{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Handling Mixed Data Types with MixedNB\n\nThis example compares three strategies for handling a dataset\nwith mixed continuous (Gaussian) and discrete (Categorical) features.\n\n**The Scenario (Generative Data):**\n\nWe generate data from two natural clusters (Classes 0 and 1):\n\n*   **Continuous Feature:** Two overlapping Gaussian distributions.\n    Precision is key here.\n*   **Categorical Feature:** Different category probabilities per class.\n    Class 0 prefers Cat '0', Class 1 prefers Cat '2'.\n\n**The Competitors:**\n\n1.  **MixedNB (Native):** Models Gaussian as Gaussian, Categorical as Multinomial.\n    Matches the data generation process. (Acc: ~0.920).\n2.  **Pipeline (OHE + GNB):** Treats categories as binary Gaussians.\n    (Acc: ~0.893).\n3.  **Pipeline (Discretizer + CatNB):** Bins continuous data.\n    (Acc: ~0.913).\n\n**Result:**\n\nMixedNB produces the smoothest probability landscape and optimal log-loss, though standard pipelines perform reasonably well on this simple problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: The scikit-bayes Developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import CategoricalNB, GaussianNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n\nfrom skbn.mixed_nb import MixedNB\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- 1. Generate Probabilistic Mixed Data ---\nnp.random.seed(42)\nn_samples = 1000\n\n# Class 0: Centered at X=-1, Prefer Cat 0\nn0 = n_samples // 2\nX_cont_0 = np.random.normal(-1.0, 1.0, size=n0)\n# Probabilities for Cat 0, 1, 2: [0.7, 0.2, 0.1]\nX_cat_0 = np.random.choice([0, 1, 2], size=n0, p=[0.7, 0.2, 0.1])\n\n# Class 1: Centered at X=1, Prefer Cat 2\nn1 = n_samples - n0\nX_cont_1 = np.random.normal(1.0, 1.0, size=n1)\n# Probabilities for Cat 0, 1, 2: [0.1, 0.2, 0.7]\nX_cat_1 = np.random.choice([0, 1, 2], size=n1, p=[0.1, 0.2, 0.7])\n\n# Combine\nX_cont = np.concatenate([X_cont_0, X_cont_1])\nX_cat = np.concatenate([X_cat_0, X_cat_1])\ny = np.concatenate([np.zeros(n0), np.ones(n1)]).astype(int)\n\n# Stack: [Continuous, Categorical]\nX = np.column_stack([X_cont, X_cat])\n\n# Split for valid metric calculation\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# --- 2. Define Models ---\n\n# Model 1: MixedNB\nmnb = MixedNB()\nmnb.fit(X_train, y_train)\n\n# Model 2: OHE + GaussianNB\npipe_ohe = make_pipeline(\n    ColumnTransformer(\n        [(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [1])],\n        remainder=\"passthrough\",\n    ),\n    GaussianNB(),\n)\npipe_ohe.fit(X_train, y_train)\n\n# Model 3: Discretizer + CategoricalNB\npipe_kbins = make_pipeline(\n    ColumnTransformer(\n        [\n            (\n                \"discretizer\",\n                KBinsDiscretizer(n_bins=5, encode=\"ordinal\", strategy=\"quantile\"),\n                [0],\n            )\n        ],\n        remainder=\"passthrough\",\n    ),\n    CategoricalNB(),\n)\npipe_kbins.fit(X_train, y_train)\n\nmodels = [mnb, pipe_ohe, pipe_kbins]\ntitles = [\n    \"1. MixedNB (Native)\\nCorrect Assumptions\",\n    \"2. Pipeline (OHE + GNB)\\nFlawed: Cats are Gaussians\",\n    \"3. Pipeline (Binned + CatNB)\\nFlawed: Loss of Precision\",\n]\n\n# --- 3. Visualization ---\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n\n# Grid for plotting\nh = 0.05\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = -0.5, 2.5  # Categories 0, 1, 2\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, 1))\n\n# Flatten for prediction\ngrid_X = np.c_[xx.ravel(), yy.ravel()]\n\nfor ax, model, title in zip(axes, models, titles):\n    # Predict\n    Z = model.predict_proba(grid_X)[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    # Metrics on Test Set\n    acc = accuracy_score(y_test, model.predict(X_test))\n    ll = log_loss(y_test, model.predict_proba(X_test))\n\n    # Plot Heatmap - VIRIDIS\n    # 0.0 = Purple (Class 0 zone), 1.0 = Yellow (Class 1 zone)\n    ax.imshow(\n        Z,\n        extent=(x_min, x_max, y_min, y_max),\n        origin=\"lower\",\n        cmap=\"viridis\",\n        vmin=0,\n        vmax=1,\n        aspect=\"auto\",\n        alpha=0.8,\n    )\n\n    # Overlay real data points (with jitter on Y)\n    X_plot = X_test.copy()\n    y_jit = X_plot[:, 1] + np.random.uniform(-0.2, 0.2, size=len(X_plot))\n\n    # Visual Coherence with Viridis:\n    # Class 0 -> Indigo (Matches background purple)\n    # Class 1 -> Gold (Matches background yellow)\n    # Edgecolors='w' ensures visibility even on matching backgrounds\n\n    ax.scatter(\n        X_plot[y_test == 0, 0],\n        y_jit[y_test == 0],\n        c=\"indigo\",\n        marker=\"o\",\n        s=30,\n        alpha=0.9,\n        edgecolors=\"w\",\n        linewidth=0.8,\n        label=\"Class 0\",\n    )\n\n    ax.scatter(\n        X_plot[y_test == 1, 0],\n        y_jit[y_test == 1],\n        c=\"gold\",\n        marker=\"^\",\n        s=30,\n        alpha=0.9,\n        edgecolors=\"k\",\n        linewidth=0.5,\n        label=\"Class 1\",\n    )  # Black edge for yellow points for better contrast\n\n    # Title & Metrics\n    ax.set_title(f\"{title}\\nAcc: {acc:.3f} | Log Loss: {ll:.3f}\", fontsize=12)\n    ax.set_xlabel(\"Continuous Feature\")\n    ax.set_yticks([0, 1, 2])\n\naxes[0].set_ylabel(\"Categorical Feature\")\n# Clean legend\nhandles, labels = axes[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc=\"lower center\", ncol=2, bbox_to_anchor=(0.5, 0.02))\n\nfig.suptitle(\n    \"MixedNB vs. Scikit-Learn Workarounds: Quality of Probability Landscape\",\n    fontsize=16,\n)\nplt.tight_layout()\nplt.subplots_adjust(top=0.80, bottom=0.15)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}